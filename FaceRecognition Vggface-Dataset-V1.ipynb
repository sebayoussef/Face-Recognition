{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.engine import  Model\n",
    "from keras.layers import Flatten, Dense, Input, GlobalAveragePooling2D\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface import utils\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau, TensorBoard\n",
    "from keras.optimizers import Adam, SGD, Nadam, RMSprop\n",
    "import cv2\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Weights Loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load weights of convolution layers\n",
    "from keras import backend as K\n",
    "vgg_model = VGGFace(include_top=False,weights='vggface', input_shape=(200,200,3))\n",
    "print('CNN Weights Loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200, 200, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 200, 200, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 200, 200, 64)      36928     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 100, 100, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 100, 100, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 100, 100, 128)     147584    \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 50, 50, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 50, 50, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             (None, 50, 50, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv3_3 (Conv2D)             (None, 50, 50, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 25, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 25, 25, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             (None, 25, 25, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv4_3 (Conv2D)             (None, 25, 25, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_1 (Conv2D)             (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_2 (Conv2D)             (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_3 (Conv2D)             (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool5 (MaxPooling2D)         (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 512)               9437696   \n",
      "_________________________________________________________________\n",
      "fc7 (Dense)                  (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "fc8 (Dense)                  (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 24,417,092\n",
      "Trainable params: 9,702,404\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Training FC layers while freezing remaining CNN layers\n",
    "nb_class = 4\n",
    "hidden_dim = 512\n",
    "\n",
    "last_layer = vgg_model.get_layer('pool5').output\n",
    "x = Flatten(name='flatten')(last_layer)\n",
    "x = Dense(hidden_dim, activation='relu', name='fc6')(x)\n",
    "x = Dense(hidden_dim, activation='relu', name='fc7')(x)\n",
    "out = Dense(nb_class, activation='softmax', name='fc8')(x)\n",
    "custom_vgg_model = Model(vgg_model.input, out)\n",
    "\n",
    "# Finetuning: only the last 3 fully connected layers are trained. The rest are made non-trainable.\n",
    "layer_count = 0\n",
    "for layer in custom_vgg_model.layers:\n",
    "    layer_count = layer_count+1\n",
    "for l in range(layer_count-3):\n",
    "    custom_vgg_model.layers[l].trainable=False\n",
    "    \n",
    "custom_vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading train, test and validation data\n",
    "train_data_dir = '/Users/seba/desktop/Datasets/Dataset/train'\n",
    "validation_data_dir = '/Users/seba/desktop/Datasets/Dataset/validate'\n",
    "#test_data_dir = '/Users/seba/Desktop/YaleDataset/test'\n",
    "nb_train_samples = 12\n",
    "nb_validation_samples = 12\n",
    "nb_test_samples = 8 # per class\n",
    "img_height = 200\n",
    "img_width = 200 \n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MEAN_VALUE = np.array([103.939, 116.779, 123.68])   # BGR\n",
    "def preprocess(img):\n",
    "    # img is (channels, height, width), values are 0-255\n",
    "    img = img[::-1]  # switch to BGR\n",
    "    img -= MEAN_VALUE\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 images belonging to 4 classes.\n",
      "Found 48 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# prepare data augmentation configuration\n",
    "validate_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess,\n",
    "    rescale=1./255,  ######\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    samplewise_center=False,\n",
    "    samplewise_std_normalization=False)\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess,\n",
    "    rescale=1./255,\n",
    "    featurewise_center=True, \n",
    "    featurewise_std_normalization=True,  \n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    samplewise_center=False,\n",
    "    samplewise_std_normalization=False)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height,img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = validate_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height,img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_path = '/Users/seba/desktop/Weights/weights_12.h5'\n",
    "#log_path = '/Users/seba/Desktop/Logs/log_12/'\n",
    "#checkpointer = ModelCheckpoint(filepath=weights_path, verbose=1,save_best_only = True, monitor = 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load best weights \n",
    "custom_vgg_model.load_weights(weights_path)\n",
    "\n",
    "# Compile model\n",
    "custom_vgg_model.compile(optimizer=SGD(lr=0.01, momentum=0.7, decay=1e-4), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8,patience=0, min_lr=1e-6)\n",
    "#tensorboard_callback=TensorBoard(log_dir='log_path, histogram_freq=0,write_graph=True,write_images=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seba/anaconda/lib/python2.7/site-packages/keras/preprocessing/image.py:506: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn'tbeen fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/Users/seba/anaconda/lib/python2.7/site-packages/keras/preprocessing/image.py:514: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn'tbeen fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11fe7de50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finetuning: train last 3 fully connected layers\n",
    "custom_vgg_model.fit_generator( \n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples/batch_size,\n",
    "    epochs=0,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples/batch_size)\n",
    "#callbacks=[checkpointer,tensorboard_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test network on testing data\n",
    "# Testing the model \n",
    "res = dict((v,k) for k,v in train_generator.class_indices.items())\n",
    "fil = open('/home/sebayoussef/Desktop/Predictions/predictions5', 'w+')\n",
    "\n",
    "for fn in os.listdir('/Users/seba/desktop/Datasets/Dataset/test/'):\n",
    "    img = imread('/Users/seba/desktop/Datasets/Dataset/test/'+fn,mode='RGB')\n",
    "    img = imresize(arr,size=(200,200,3))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    # preprocessing on input data\n",
    "    x = preprocess_input(x)\n",
    "    i = model.predict(x)\n",
    "    i = np.argmax(i,axis=1)[0]\n",
    "    fil.write(fn + ',' + res[i] + '\\n')\n",
    "\n",
    "fil.close()\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "    cvscores1 = []\n",
    "for train, test in kfold.split(X_reduced,Y):\n",
    "    scores1 = model.evaluate(X_reduced[test], Y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores1[1]*100))\n",
    "    cvscores1.append(scores1[1] * 100)\n",
    "print(\"Average Accuracy: %.2f%% \" % (np.mean(cvscores1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-d574d14e8981>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-d574d14e8981>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    >>> x = np.random.randint(0,10,(5,5,3))\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "res = dict((v,k) for k,v in train_generator.class_indices.items())\n",
    "print res\n",
    "\n",
    "\n",
    "\n",
    "file=cv2.imread(filename)\n",
    "file=cv2.resize(file,(224,224))\n",
    "file=cv2.cvtColor(file, cv2.COLOR_BGR2RGB)\n",
    "file=np.array(image).reshape((3,224,224))\n",
    "files.append([file])\n",
    "print(file.shape[0])\n",
    ">>> x = np.random.randint(0,10,(5,5,3))\n",
    "    >>> x.shape\n",
    "    >>> (5, 5, 3)\n",
    "    >>> x = np.expand_dims(x, axis=0)\n",
    "    >>> x.shape\n",
    "    >>> (1, 5, 5, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Maryam', 1: 'Seba', 2: 'Tarek', 3: 'Youssef'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = dict((v,k) for k,v in train_generator.class_indices.items())\n",
    "res1 = np.array(res)\n",
    "x = np.expand_dims(res1, axis=0)\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "\n",
    "x=[1,1,]\n",
    "x = np.asarray(x)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x.shape\n",
    "y_test = keras.utils.to_categorical(np.random.randint(4, size=(1, 1)), num_classes=4)\n",
    "y_test[0][:]=1\n",
    "\n",
    "print y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 0.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n",
      "(1, 200, 200, 3)\n",
      "acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "label=-1\n",
    "y_test = keras.utils.to_categorical(np.random.randint(4, size=(1, 1)), num_classes=4)  \n",
    "\n",
    "avgscore=[]\n",
    "for dir in os.listdir('/Users/seba/desktop/Datasets/Dataset/test/'):\n",
    "    if (dir!='.DS_Store'):\n",
    "        label=label+1\n",
    "        y_test[0][:]=0   #initialize all labels to zero\n",
    "        y_test[0][label]=1  # current label\n",
    " \n",
    "        for fn in os.listdir('/Users/seba/desktop/Datasets/Dataset/test/'+dir):\n",
    "            if fn.endswith(\".jpg\"):\n",
    "                img = cv2.imread('/Users/seba/desktop/Datasets/Dataset/test/'+dir+'/'+fn,1) # read as RGB \n",
    "                img = cv2.resize(img, (200,200))\n",
    "                x = np.asarray(img, dtype='float32')\n",
    "                x = preprocess(x)\n",
    "                # rescaling\n",
    "                x= x* 1./255\n",
    "                x = np.expand_dims(x, axis=0)\n",
    "                print x.shape\n",
    "                # preprocessing on input data\n",
    "                score = custom_vgg_model.evaluate(x,y_test,verbose=0)\n",
    "                #print score\n",
    "                avgscores.append(score[1] * 100)\n",
    "\n",
    "                print(\"%s: %.2f%%\" % (custom_vgg_model.metrics_names[1], score[1]*100))\n",
    "                \n",
    "######## shape of label ??????\n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train last FC layer or train whole network including CNN layers? \n",
    "# Validation_steps??\n",
    "# Subtract mean of images?? \n",
    "\n",
    "\n",
    "#The number of samples processed for each epoch is batch_size * steps_per_epochs.\n",
    "#steps_per_epoch = number of times weights are updated per epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
